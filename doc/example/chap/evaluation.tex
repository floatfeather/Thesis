\chapter{实验与验证}

\section{研究问题}

本文提出了一个结合了基于频谱的缺陷定位和基于状态覆盖的缺陷定位的缺陷定位框架，
并且通过结合深入分析基于频谱的缺陷定位和基于状态覆盖的缺陷定位能够准确定位的原因。

本文试图回答以下几个问题：
\begin{enumerate}
\item \textbf{当使用不同的数据收集粒度时，缺陷定位技术的效果有什么不同？} \\
这个研究问题探索了数据收集的粒度对缺陷定位结果的影响。
本文主要考虑两种数据收集粒度，
第一种是谓词的数量，第二种是程序元素的大小。
具体来说，对于谓词数量，本文考虑不同谓词数量下的语句级别的缺陷定位效果的不同。
对于程序元素大小，本文考虑的是语句级别和方法级别的数据收集对缺陷定位效果的影响。
\item \textbf{不同的怀疑度计算公式会如何影响缺陷定位的结果？} \\
这个研究问题探索了缺陷定位公式的重要性，
并分析了是否还能通过改进缺陷定位公式去提升缺陷定位效果，
为后续研究提供了参考。
\item \textbf{缺陷定位技术的不同结合方式效果如何？} \\
在章\ref{sec:approach_comb}中，
本文探讨了 PREDSBFL 和COMBSD 两种结合缺陷定位技术的方法，
并且每种结合方法还有多种不同的结合方式。
这个研究问题研究了不同结合方式的效果。
\item \textbf{结合后的缺陷定位技术与现有技术相比，效果如何？} \\
在这个研究问题中，
本文提出的结合的缺陷定位技术将与现有技术进行比较，并对其结果进行分析。
\item \textbf{机器学习模型预测谓词的准确率如何？} \\
这个研究问题探索了谓词预测模型的准确率，
并且在不同的机器学习模型上进行了对比。
\item \textbf{机器学习模型预测的谓词能否帮助有效定位缺陷？} \\
这个研究问题比较了基于谓词预测的缺陷定位和本文提出的结合方法、现有方法之间的结果，
分析预测的谓词在定位缺陷中的作用和不足。
\end{enumerate}

\section{实验数据}

本文使用Defects4j数据集\parencite{Just2014Defects4J}（v1.0）作为实验对象，
下文所提到的Defects4j都是v1.0版本。
Defects4j数据集含有357个真实的缺陷，来自五个大型的开源软件项目，见表\ref{defects4j_details}。
其中，“实验缺陷数目”是本文在实验中实际使用了的缺陷数目，“代码行数（千行）”表示的是这五个项目最近的版本的代码行数。
本文只使用了330个缺陷是因为其余27个缺陷因为插装后的代码太大导致“code too large"的报错而无法运行。

\begin{table}
\centering
\begin{tabular}{|l|r|r|r|r|}
\hline
项目名称 & 缺陷数目 & 实验缺陷数目 & 代码行数（千行） & 测试用例数目 \\
\hline
JFree\textbf{Chart} & 26 & 26 & 96 & 2205 \\
\hline
\textbf{Closure} compiler & 133 & 122 & 90 & 7927 \\
\hline
Apache commons-\textbf{Math} & 106 & 98 & 85 & 3602 \\
\hline
Apache commons-\textbf{Lang} & 65 & 57 & 22 & 2245 \\
\hline
Joda-\textbf{Time} & 27 & 27 & 28 & 4130 \\
\hline
总计 & 357 & 330 & 321 & 20109 \\
\hline
\end{tabular}
\caption{Defects4j数据集缺陷情况}
\label{defects4j_details}
\end{table}

\section{实验标准}

为了验证本文提出的结合后的缺陷定位的效果，以及预测谓词的效果，
需要一套衡量缺陷定位效果的标准和预测谓词效果的标准。

\subsection{缺陷定位结果标准}

本文使用两种常用的缺陷定位标准： Top-k 的召回率和 EXAM 分数。

Top-k 的召回率衡量的是有多少缺陷能够排在怀疑列表的前k位。
根据Kochhar等人的研究\parencite{Kochhar2016Practitioners}，
超过73\%的参与者认为观察怀疑列表的前5个程序元素是可以接受的，
几乎所有参与者都认为10个程序元素是可以接受的最大的需要观察的程序元素个数。
所以本文采用1，3，5，10作为k值。
对于怀疑度分数相同的程序元素，它们的排名会使用平均排名，这也是很多已有研究使用的方法\parencite{Pearson2017Evaluating,Xuan2014Learning,Steimann2013Threats,Wong2016A}。

EXAM 分数衡量是开发者需要查看多少个位置才能看到真正的缺陷。
$$
\mathrm{EXAM} = \frac{n}{N}
$$
其中$N$表示候选的程序元素个数（比如被失败的测试用例覆盖的语句），$n$表示缺陷程序元素排在怀疑度列表的第$n$位。
EXAM 分数是一个0到1之间的值，且值越小越好。
它反映出了含有缺陷的程序元素在所有可疑程序元素中的相对位置，
体现出整个缺陷定位方法的效果。
很多工作都使用了这个方法\parencite{Wong2012Effective,Pearson2017Evaluating}。

\subsection{谓词预测结果标准}

本文对谓词预测结果的评判方法采用机器学习分类问题的评判方法。

首先引入一些评估分类问题效果时需要的统计量。
混淆矩阵统计量见表\ref{confusion_matrix}。
样本总数为$N$。
正例反例为两个相对的概念。
对于多分类问题，其余分类都为反例。

\begin{table}
\centering
\begin{tabular}{|c|c|c|}
\hline
\multirow{2}*{真实情况} & \multicolumn{2}{|c|}{预测结果} \\
\cline{2-3}
~ & 正例 & 反例 \\
\hline
正例 & TP（真正例） & FN（假反例） \\
\hline
反例 & FP（假正例） & TN（真反例） \\
\hline
\end{tabular}
\caption{分类结果混淆矩阵}
\label{confusion_matrix}
\end{table}

准确率（accuracy）衡量的是分类正确的样本数占样本总数的比例：
$$
\mathrm{Accuracy} = \frac{TP + TN}{N} = \frac{TP + TN}{TP + FN + FP + TN}
$$
。

准确率能够衡量一个机器学习模型的预测情况，但是有的时候只有准确率还不够。
有的时候人们还关心查准率和召回率。

查准率（precision）衡量的是被分类为正例的样本中有多少是真的正例：
$$
\mathrm{Precision} = \frac{TP}{TP + FP}
$$。

查全率或召回率（recall）衡量的是正例中有多少被分类为正例：
$$
\mathrm{Recall} = \frac{TP}{TP + FN}
$$。

查准率和查全率是一对相互矛盾的度量。
一般来说，查准率高时，查全率低，而查准率低时，查全率高。
F1 度量结合查准率和查全率，给出一个便于比较的统一的值：
$$
\mathrm{F1} = \frac{2 \times \mathrm{Precision} \times \mathrm{Recall}}{\mathrm{Precision} + \mathrm{Recall}}
$$。

在本文的问题中，VAR模型是一个二分类模型，可以使用准确率、查准率、召回率和F1评判。
EXPR模型是一个多分类模型，而且分类数量往往是四位数，使用查准率和召回率会导致查准率和召回率数量非常大
（其数量和分类数量一致）。
所以本文对VAR模型使用准确率、查准率、召回率和F1标准，
而对EXPR模型只使用准确率标准。

\section{实验结果与分析}

